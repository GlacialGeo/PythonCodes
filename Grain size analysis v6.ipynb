{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6024934",
   "metadata": {},
   "source": [
    "Code by Dr. Martin Ross(1), modified from previous versions that were written with the assistance of Cindy Liu(2) <br>\n",
    "ChapGPT (GPT-4) was also used to fix some issues and improve a few things <br>\n",
    "(1) Associate Professor, Earth and Environmental Sciences <br>\n",
    "(2) Coop student, Physics & Astronomy <br>\n",
    "University of Waterloo, Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52256443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The default structure is samples (rows) and weight frequencies (cols) from coarse (left) to fine (right)\n",
    "#The grain size headers and grain size values are in millimetres (see note below for laser data in microns)\n",
    "#A minimum of two samples/rows are expected\n",
    "#Rename file and adjust the skiprows and column range as needed\n",
    "\n",
    "#NOTES:\n",
    "#For laser data in microns, there is an option below [cell 9] to adjust phival calculations\n",
    "\n",
    "#For sieve data, the wt. Freq. of the pan fraction in the input file should be extrapolated and distributed across the full range of silt and clay in a decreasing fashion\n",
    "#This can be improved by analyzing the fine fraction with a another method like laser diffractomery\n",
    "\n",
    "# Read the entire file into a list of lines\n",
    "with open(\"Normal_grain_size_data.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Determine the number of columns in the third row (excluding the first column)\n",
    "first_data_row = lines[2].strip().split()\n",
    "num_columns = len(first_data_row)\n",
    "\n",
    "# Initialize lists to store data\n",
    "allFreq = []\n",
    "grainSizesData = []\n",
    "\n",
    "# Process the file line by line\n",
    "for i, line in enumerate(lines):\n",
    "    # Skip the first two rows for 'allFreq' and 'allLabels'\n",
    "    if i >= 2:\n",
    "        columns = line.strip().split()\n",
    "        allFreq.append(columns[1:num_columns])\n",
    "\n",
    "    # Extract grain sizes from the second row\n",
    "    if i == 1:\n",
    "        grainSizesData = line.strip().split()[0:num_columns]\n",
    "\n",
    "# Convert `allFreq` to a numpy array and transpose it\n",
    "allFreq = np.array(allFreq, dtype=float).transpose()\n",
    "\n",
    "# Desired number of decimal places for rounding\n",
    "decimal_places = 4\n",
    "\n",
    "# Round grain size labels in mm\n",
    "grainSizes = [round(float(size), decimal_places) for size in grainSizesData]\n",
    "\n",
    "# Extract sample IDs from the third row onwards\n",
    "allLabels = np.array([line.strip().split()[0] for line in lines[2:]], dtype='str')\n",
    "\n",
    "#print(\"allFreq:\\n\", allFreq)\n",
    "#print(\"grainSizes:\", grainSizes)\n",
    "#print(\"allLabels:\", allLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e67abd-13a7-4e9c-8b24-6a0c4e7307b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(allFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grainSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative(allSamples, start, end):\n",
    "    # inputs: (list of lists, int, int)\n",
    "    sampleCumulative = [[]] * (end - start)\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        sampleCumulative[i - start] = allSamples[:, i]\n",
    "        sampleCumulative[i - start] = np.cumsum(sampleCumulative[i - start])\n",
    "        \n",
    "    return sampleCumulative\n",
    "\n",
    "def cumulativeCurve(allSamples, start, end, xSize, ySize):\n",
    "    if allSamples.size == 0:\n",
    "        print(\"The dataset is empty\")\n",
    "        return\n",
    "\n",
    "    cumulativeVals = cumulative(allSamples, start, end)\n",
    "    \n",
    "    for i in range(len(cumulativeVals)):\n",
    "        color = '#465775' if (i % 2) == 0 else '#ef6f6c'\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(xSize, ySize))\n",
    "        plt.plot(range(len(grainSizes)), cumulativeVals[i], \n",
    "                 color=color, linewidth=2.5, marker='o')\n",
    "        plt.xticks(np.arange(0, len(phiVals)), labels=phiVals, fontsize=14)\n",
    "        plt.ylim((-0.5, 105))\n",
    "        ax.set_xlabel('phi', fontsize=14)\n",
    "        ax.set_ylabel('Cumulative wt. Frequency %', fontsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_title('Cumulative wt. Frequency of Sample ' + str(allLabels[start + i]), \n",
    "                     pad=20, fontsize=18)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06721c76-692a-4afa-882c-54638f3bddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives the option of presenting the cumulative curve on a normal probability scale (y axis)\n",
    "\n",
    "def cumulativeCurveProb(allSamples, start, end, xSize, ySize):\n",
    "    # inputs: (list of lists, int, int, int, int)\n",
    "    \n",
    "    cumulativeVals = cumulative(allSamples, start, end)\n",
    "    \n",
    "    # Define fixed y-ticks for probability scale (e.g., 0.1%, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%)\n",
    "    fixed_percentiles = np.array([0.1, 1, 5, 10, 25, 50, 75, 90, 95, 99])\n",
    "    y_ticks = norm.ppf(fixed_percentiles / 100.0)\n",
    "    y_tick_labels = [f'{percentile:.1f}%' for percentile in fixed_percentiles]\n",
    "    \n",
    "    # Define fixed y-axis limits\n",
    "    y_min = norm.ppf(0.0001)  # Minimum value corresponding to 0.01%\n",
    "    y_max = norm.ppf(0.9999)   # Maximum value corresponding to 99.99%\n",
    "    \n",
    "    for i in range(len(cumulativeVals)):\n",
    "        if (i % 2) == 0:\n",
    "            colour = '#465775'\n",
    "        else:\n",
    "            colour = '#ef6f6c'\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(xSize, ySize))\n",
    "        \n",
    "        # Transform cumulative values to a probability scale\n",
    "        cumulative_percentage = np.array(cumulativeVals[i])\n",
    "        prob_scale_values = norm.ppf(cumulative_percentage / 100.0)\n",
    "        \n",
    "        plt.plot(range(len(grainSizes)), prob_scale_values, \n",
    "                 color=colour, linewidth=2.5, marker='o')\n",
    "        plt.xticks(np.arange(0, len(phiVals)), labels=phiVals, fontsize=14)\n",
    "\n",
    "        # Set y-axis to probability scale\n",
    "        ax.set_yscale('linear')\n",
    "\n",
    "        # Apply fixed y-ticks and labels\n",
    "        ax.set_yticks(y_ticks)\n",
    "        ax.set_yticklabels(y_tick_labels)\n",
    "        \n",
    "        # Set fixed y-axis limits\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "        ax.set_xlabel('phi', fontsize=14)\n",
    "        ax.set_ylabel('Cumulative Probability (%)', fontsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_title('Cumulative Probability of Sample ' + str(allLabels[start + i]), \n",
    "                     pad=20, fontsize=18)\n",
    "        plt.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulativeCurve2(allSamples, start, end, xSize, ySize):\n",
    "    \n",
    "    cumulativeVals = cumulative(allSamples, start, end)\n",
    "    \n",
    "    # Define a list of colors and line styles\n",
    "    colors = ['#A0522D', '#708090', '#2E8B57', '#000000', '#FFA500']\n",
    "    line_styles = ['-', '--', '-.', ':', (0, (3, 5, 1, 5))]\n",
    "    markers = ['.', 'o', 'x', '+', '^']\n",
    "    \n",
    "    # Create a single figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(xSize, ySize))\n",
    "    \n",
    "    for i, (color, style, marker) in enumerate(zip(colors, line_styles, markers)):\n",
    "        if i >= len(cumulativeVals):\n",
    "            break  # Exit loop if there are more styles than curves\n",
    "        \n",
    "        # Plot the cumulative curve on the same axis with unique style and color\n",
    "        ax.plot(range(len(grainSizes)), cumulativeVals[i], \n",
    "                color=color, linestyle=style, linewidth=2.5, marker=marker, \n",
    "                markersize=8,  # Adjust marker size here\n",
    "                label=f'{allLabels[start+i]}')\n",
    "    \n",
    "    plt.xticks(np.arange(0, len(phiVals)), labels=phiVals, fontsize=14)\n",
    "    plt.ylim((-0.5, 105))\n",
    "    ax.set_xlabel('phi', fontsize=14)\n",
    "    ax.set_ylabel('Cumulative wt. Frequency %', fontsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_title('Cumulative wt. Frequency of Selected Samples', \n",
    "                 pad=20, fontsize=18)\n",
    "    ax.legend()  # Show legend with sample labels\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating phi values\n",
    "grainSizesFloat = []\n",
    "\n",
    "for i in grainSizes[:]:\n",
    "    grainSizesFloat.append(float(i))\n",
    "\n",
    "phiVals = []\n",
    "\n",
    "for i in grainSizesFloat:\n",
    "    phi = -float(round(math.log(i,2),2))\n",
    "    if phi == -0.0:\n",
    "        phi = 0.0\n",
    "    phiVals.append(phi)\n",
    "\n",
    "#Use this one for laser data in microns\n",
    "#for i in grainSizesFloat:\n",
    "    #phi = -float(round(math.log(i/1000,2), 2))\n",
    "    #if phi == -0.0:\n",
    "        #phi = 0.0\n",
    "    #phiVals.append(phi)\n",
    "\n",
    "print(phiVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb128e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for verbal representations of skew and standard deviation\n",
    "def verbalSkew(skewVal):\n",
    "    if skewVal > 0.3:\n",
    "        skewText = \"Strongly fine-skewed\"\n",
    "    elif skewVal >= 0.1:\n",
    "        skewText = \"Fine skewed\" \n",
    "    elif skewVal >= -0.1:\n",
    "        skewText = \"Near symmetrical\"\n",
    "    elif skewVal >=-0.3:\n",
    "        skewText = \"Coarse skewed\"\n",
    "    else:\n",
    "        skewText = \"Strongly coarse skewed\"\n",
    "    return skewText\n",
    "\n",
    "def verbalStdev(stdev):\n",
    "    if stdev >4.00:\n",
    "        stdevText = \"Extremely poorly sorted\"\n",
    "    elif stdev >=2.0:\n",
    "        stdevText = \"Very poorly sorted\"\n",
    "    elif stdev >=1.00:\n",
    "        stdevText = \"Poorly sorted\"\n",
    "    elif stdev >=0.71:\n",
    "        stdevText = \"Moderately sorted\"\n",
    "    elif stdev >=0.50:\n",
    "        stdevText = \"Moderately well sorted\"\n",
    "    elif stdev >=0.35:\n",
    "        stdevText = \"Well sorted\"\n",
    "    else:\n",
    "        stdevText = \"Very well sorted\"  \n",
    "    return stdevText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for skewness, mean, and stdev\n",
    "def skewMeanStdev(cumulativeVals):\n",
    "    \n",
    "    # inputs: (list)\n",
    "    \n",
    "    skews = []\n",
    "    means = []\n",
    "    stdevs = []\n",
    "    \n",
    "    for i in range(len(cumulativeVals)):\n",
    "        phi5p = np.interp(5, cumulativeVals[i], phiVals)\n",
    "        phi16p = np.interp(16, cumulativeVals[i], phiVals)\n",
    "        phi50p = np.interp(50, cumulativeVals[i], phiVals)\n",
    "        phi84p = np.interp(84, cumulativeVals[i], phiVals)\n",
    "        phi95p = np.interp(95, cumulativeVals[i], phiVals)\n",
    "        \n",
    "        skew = ((phi84p+phi16p-(2*phi50p))/(2*(phi84p-phi16p))) + ((phi95p+phi5p-(2*phi50p))/(2*(phi95p-phi5p)))\n",
    "        mean = (phi16p+phi50p+phi84p)/3\n",
    "        stdev = ((phi84p-phi16p)/4) + ((phi95p-phi5p)/6.6)\n",
    "        \n",
    "        skews.append(skew)\n",
    "        means.append(mean)\n",
    "        stdevs.append(stdev)\n",
    "        \n",
    "    return [skews, means, stdevs]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cfaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(allSamples, allLabels, start, end):\n",
    "    cumulativeVals = cumulative(allSamples, start, end)\n",
    "    statsVals = skewMeanStdev(cumulativeVals)\n",
    "    skews = statsVals[0]\n",
    "    means = statsVals[1]\n",
    "    stdevs = statsVals[2]\n",
    "\n",
    "    df = pd.DataFrame({'Sample ID': allLabels[start:end], \n",
    "                       'Mean': means, \n",
    "                       'Skewness': skews, \n",
    "                       'Standard Deviation': stdevs})\n",
    "    \n",
    "    # add verbal skewness and standard deviation columns\n",
    "    df['Verbal Skewness'] = df['Skewness'].apply(verbalSkew)\n",
    "    df['Verbal Stdev'] = df['Standard Deviation'].apply(verbalStdev)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "createDataFrame(allFreq,allLabels, 0, allFreq.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate weight frequency distribution graphs \n",
    "def wtHist(allSamples, start, end, xSize, ySize):\n",
    "    \n",
    "    '''\n",
    "    inputs: (list of lists, int, int, list of \n",
    "            lists, int, int)\n",
    "            \n",
    "    start>end\n",
    "    '''\n",
    "    adjust = start\n",
    "    if start > 5:\n",
    "        adjust -= 1\n",
    "    \n",
    "    specifiedSamples = [[]] * (end-start)\n",
    "    for i in range(start,end):\n",
    "        specifiedSamples[i-start] = allSamples[:,i]\n",
    "    \n",
    "    sampleCount = start\n",
    "    \n",
    "    for i in specifiedSamples:\n",
    "        if (sampleCount % 2) == 0:\n",
    "            colour = '#4464ad'\n",
    "        else:\n",
    "            colour = '#a4b0f5'\n",
    "        fig, ax = plt.subplots(1,1,figsize=(xSize,ySize))\n",
    "        ax.bar(np.arange(len(i)), i, color=colour, alpha=0.8)\n",
    "        ax.plot(np.arange(len(i)), i, color='#f58f29', \n",
    "                marker='o', linewidth=0)\n",
    "        plt.xticks(np.arange(0,len(grainSizes))+0.5, labels=grainSizes)\n",
    "        ax.set_xlabel('Grain Size (mm)', fontsize=14)\n",
    "        ax.set_ylabel('Frequency (wt. %)', fontsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "        ax.set_title('Weight Frequency Distribution of Sample '\n",
    "                     + str(allLabels[sampleCount]), pad=20, fontsize=18)\n",
    "        plt.show()\n",
    "        sampleCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight frequency distribution of samples 1-5\n",
    "wtHist(allFreq,0,2, 14,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to create the figures and save each figure to the PDF file\n",
    "cumulativeCurve(allFreq, 0, 2, 10, 7) #cumulative curves of grain sizes in samples 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d7138-3b4c-48f8-aa46-ac8bcc1d3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to create the figures and save each figure to the PDF file\n",
    "cumulativeCurveProb(allFreq, 0, 2, 10, 10) #cumulative curves of grain sizes in samples 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdab010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to create the figures and save each figure to the PDF file\n",
    "cumulativeCurve2(allFreq, 0, 2, 10, 7) #cumulative curves of grain sizes in samples 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86652ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
